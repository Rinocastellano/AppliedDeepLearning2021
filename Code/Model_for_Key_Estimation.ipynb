{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m7cuRoUW5hmD"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, splitext\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l1\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras import Input\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uchZddi85oRD",
    "outputId": "7e8c73d7-b033-4282-8184-eb5c1dc73715"
   },
   "outputs": [],
   "source": [
    "#If you didn't run the previous code that do the preprocessing -> run this part of code please\n",
    "#DATASET USED FOR THE MODEL FOR TEMPO ESTIMATION\n",
    "!gdown --id 17T-UI5zb5WCBga4jZzKC-sF4z4loDFr1\n",
    "!gdown --id 1MgrEAf7k2CM0POlwmrBuK4nJlp_ciIoY\n",
    "\n",
    "path=['Image_spec2.zip','Annotations.zip']\n",
    "path_images=[\"Image_spec2/\"]\n",
    "path_ann=[\"Annotations/\"]\n",
    "from zipfile import ZipFile\n",
    "for p in path:\n",
    "    with ZipFile(p, 'r') as zipp:\n",
    "        # printing all the contents of the zip file\n",
    "        #zip.printdir()\n",
    "        print('Extracting all the files now...')\n",
    "        zipp.extractall()\n",
    "        print('Done!')\n",
    "flag=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DpDOruI06Jta"
   },
   "outputs": [],
   "source": [
    "#If you have done the preprocessing using the previous notebook, please run this part of code\n",
    "path_images=[\"Dataset/Key/GS/Image_spec\",\"Dataset/Key/GTZAN/Image_spec\"]\n",
    "path_ann=[\"Dataset/Key/GS/Ann\",\"Dataset/Key/GTZAN/Ann\"]\n",
    "flag=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6L84lF36VdE",
    "outputId": "9342df9e-d04b-45f3-978f-b71d6e27406f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16940\n",
      "Sorting both images and annotations...\n",
      "Processing the images...\n",
      "Processing the annotations\n",
      "Split a total of 16940 samples into three sets with 10164 training, 3388 validation and 3388 test samples.\n"
     ]
    }
   ],
   "source": [
    "def split_train_test_validation(path_label,path_img):\n",
    "    f_l=[]\n",
    "    f_i=[]\n",
    "    \n",
    "    for i,l in zip(path_img,path_label):\n",
    "        f_l.extend([os.path.join(root, name) for root, dirs, files in os.walk(l) for name in files if name.endswith((\".txt\"))])\n",
    "        f_i.extend([os.path.join(root, name) for root, dirs, files in os.walk(i) for name in files if name.endswith((\".jpeg\"))]) \n",
    "    if flag==0:\n",
    "\n",
    "        f_l=[os.path.join(f.split('/')[1],f.split('/')[2]) for f in f_l]\n",
    "        f_i=[os.path.join(f.split('/')[1],f.split('/')[2]) for f in f_i]\n",
    "    else:\n",
    "\n",
    "        f_l=[f.split('/')[2]+'/'+(f.split('/')[3]).split('\\\\')[1] for f in f_l]\n",
    "        f_i=[f.split('/')[2]+'/'+(f.split('/')[3]).split('\\\\')[1] for f in f_i]\n",
    "    \n",
    "    f_l= np.asarray(f_l)\n",
    "    f_i= np.asarray(f_i)\n",
    "    print(\"Sorting both images and annotations...\")\n",
    "    for n in range(f_l.shape[0]):\n",
    "        while True:\n",
    "            \n",
    "            s=splitext(f_l[n])[0]\n",
    "            if \"lerch\" in s:\n",
    "                s=s[:len(s)-6]\n",
    "            \n",
    "            res=np.where(f_i==f\"{str(s)}.jpeg\")\n",
    "            if(n==res[0]):\n",
    "                break\n",
    "            f_i[n], f_i[res[0][0]] = f_i[res[0][0]], f_i[n]\n",
    "    c=list(zip(f_l,f_i))\n",
    "    random.shuffle(c)\n",
    "    f_l,f_i=zip(*c)\n",
    "    if flag==1:\n",
    "        f_l=[\"Dataset/Key/\"+f.split('/')[0]+'/Ann/'+f.split('/')[1] for f in f_l]\n",
    "        f_i=[\"Dataset/Key/\"+f.split('/')[0]+'/Image_spec/'+f.split('/')[1] for f in f_i]\n",
    "    images=[]\n",
    "    ann=[]\n",
    "    print(\"Processing the images...\")\n",
    "    for f in f_i:\n",
    "        if flag==0:\n",
    "            images.append(np.asarray(ImageOps.grayscale(Image.open(f\"Image_spec2/{f}\").resize((40,256)))))\n",
    "        if flag==1:\n",
    "            images.append(np.asarray(ImageOps.grayscale(Image.open(f).resize((40,256)))))\n",
    "    print(\"Processing the annotations\")\n",
    "    \n",
    "    for an in f_l:\n",
    "        if flag==0:\n",
    "            with open(f\"Annotations/{an}\") as f:\n",
    "                ann.append(int(f.readlines()[0]))\n",
    "        if flag==1:\n",
    "            with open(an) as f:\n",
    "                ann.append(int(f.readlines()[0]))\n",
    "    test_x=np.asarray(images[0:math.ceil(len(f_i)*0.2)])\n",
    "    test_file=f_i[:math.ceil(len(f_i)*0.2)]\n",
    "    test_y=np.array(ann[0:math.ceil(len(f_i)*0.2)])\n",
    "    train_x=np.asarray(images[len(test_x):len(test_x)+math.ceil(len(f_i)*0.6)])\n",
    "    train_y=np.array(ann[len(test_x):len(test_x)+math.ceil(len(f_i)*0.6)])\n",
    "    val_x=np.asarray(images[len(test_x)+len(train_x):])\n",
    "    val_y=np.array(ann[len(test_x)+len(train_x):])\n",
    "    train_y = to_categorical(train_y, 24)\n",
    "    test_y = to_categorical(test_y, 24) \n",
    "    val_y= to_categorical(val_y, 24)\n",
    "    print(f\"Split a total of {len(f_i)} samples into three sets with {len(train_x)} training, {len(val_x)} validation and {len(test_x)} test samples.\")\n",
    "    return test_x,test_y,train_x,train_y,val_x,val_y,test_file\n",
    "\n",
    "\n",
    "x_test,y_test,x_train,y_train,x_val,y_val,test_file=split_train_test_validation(path_ann,path_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2v8Qygew6udY",
    "outputId": "2adc34f9-3bb9-4cf6-96c6-e016696daa83"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras import Input\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(16, (5, 5), activation='relu', input_shape=(256, 40,1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (5, 5), activation='relu',kernel_regularizer=keras.regularizers.l2(0.005)), \n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(16, (5, 5), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (5, 5), activation='relu',kernel_regularizer=keras.regularizers.l2(0.005)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='relu',kernel_regularizer=keras.regularizers.l2(0.005)),\n",
    "    layers.Dense(24, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.055),loss='categorical_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=75, batch_size=132,validation_data=(x_val, y_val),callbacks=[es,mc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9F5T1UH8Z8S"
   },
   "outputs": [],
   "source": [
    "pyplot.plot(history.history['acc'], label='train')\n",
    "pyplot.plot(history.history['val_acc'], label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xFgzOcFV8gtr",
    "outputId": "71de585b-7bec-4d05-ff66-eada66e77835"
   },
   "outputs": [],
   "source": [
    "#the accuracy is defined as the readme explain:\n",
    "#Relative fifth: error of 50% (Ex. Label: C major , Prediction: G major)\n",
    "#Relative minor/major: error of 75% (Ex. Label: C major, Prediction: A minor)\n",
    "#Parallel minor/major: error of 90% (Ex. Label: C major, Prediciotn: C minor) \n",
    "def get_accuracy(model, true_x, true_y): \n",
    "    res = model.predict(true_x)\n",
    "    res = np.argmax(res, axis=-1)\n",
    "    acc = 0\n",
    "    for i in range(len(true_y[:, 0])):\n",
    "        r=res[i]\n",
    "        t=np.where(true_y[i,:]==1)[0]\n",
    "        if r==t:\n",
    "          \n",
    "          acc+=1\n",
    "        elif r==(t+7)% (24//2):\n",
    "          acc+=0.5\n",
    "        elif r==(t-3) or r==(t+9)%24 or r==(t+3)%(24//2):\n",
    "          acc+=0.25\n",
    "        elif (r==(t+12)%24):\n",
    "          acc+=0.1\n",
    "        else:\n",
    "            pass\n",
    "    tot = len(true_y[:,0])\n",
    "    print('True - total', acc, tot)\n",
    "    print('acc: {}'.format((acc/tot)))\n",
    "\n",
    "    \n",
    "#This function is implemented in order to have the training dataset distinguished for the original dataset, indeed\n",
    "#the output are the ExtendedBallroom and Giantsteps dataset\n",
    "def get_dataset(label,img,test_file):\n",
    "  f_l=[f for f in listdir(\"Annotations\") if isfile(join(\"Annotations\", f))]\n",
    "  f_i=[f for f in listdir(\"Image_spec2\") if isfile(join(\"Image_spec2\", f))]\n",
    "  f_l= np.asarray(f_l)\n",
    "  f_i= np.asarray(f_i)\n",
    "  for n in range(f_l.shape[0]):\n",
    "    print(n)\n",
    "    while True:\n",
    "      s=splitext(f_l[n])[0]\n",
    "      if \"lerch\" in s:\n",
    "        s=s[:len(s)-6]\n",
    "      res=np.where(f_i==f\"{str(s)}.jpeg\")\n",
    "      if(n==res[0]):\n",
    "        break\n",
    "      f_i[n], f_i[res[0][0]] = f_i[res[0][0]], f_i[n]\n",
    "  c=list(zip(f_l,f_i))\n",
    "  giant_i=[]\n",
    "  gtzan_i=[]\n",
    "  gtzan_l=[]\n",
    "  giant_l=[]\n",
    "  lofi=\"LOFI\"\n",
    "  words=['blues','classical','country','disco','hiphop','jazz','metal','rock','reggae','pop']\n",
    "  for f in f_i:\n",
    "    if f in test_file: \n",
    "      if lofi in f:\n",
    "        idx=np.where(f_i == f)\n",
    "        giant_i.append(f)\n",
    "        giant_l.append(f_l[idx])\n",
    "      for w in words:\n",
    "        if w in f:\n",
    "          idx=np.where(f_i == f)\n",
    "          gtzan_i.append(f)\n",
    "          gtzan_l.append(f_l[idx])\n",
    "  gt_i=[]\n",
    "  gt_l=[]\n",
    "  gi_i=[]\n",
    "  gi_l=[]\n",
    "  for g1 in gtzan_i:\n",
    "    gt_i.append(np.asarray(ImageOps.grayscale(Image.open(f\"Image_spec2/{g1}\").resize((100,100)))))\n",
    "  for g2 in giant_i:  \n",
    "    gi_i.append(np.asarray(ImageOps.grayscale(Image.open(f\"Image_spec2/{g2}\").resize((100,100)))))\n",
    "  for g1 in gtzan_l:\n",
    "    with open(f\"Annotations/{g1[0]}\") as f:\n",
    "      gt_l.append(int(f.readlines()[0]))\n",
    "  for g2 in giant_l:\n",
    "    with open(f\"Annotations/{g2[0]}\") as f:\n",
    "      gi_l.append(int(f.readlines()[0]))\n",
    "  gt_i=np.asarray(gt_i)\n",
    "  gt_l=np.asarray(gt_l)\n",
    "  gi_i=np.asarray(gi_i)\n",
    "  gi_l=np.asarray(gi_l)\n",
    "  return gt_i,gt_l,gi_i,gi_l\n",
    "gtzan_i,gtzan_l,giant_i,giant_l=get_dataset(path_ann,path_images,test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(model,giant_i,giant_l)\n",
    "get_accuracy(model,gtzan_i,gtzan_l)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model for Key Estimation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
